{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "mount_file_id": "17N2N9L7teg-4JfvyVUgYvdcJCMURjy4H",
      "authorship_tag": "ABX9TyPZujTZvi1FCqeWBT542DpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fathursidiq/CNNtampalibrary/blob/main/SegmentaionsBiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEXi6ij5kbYo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tensorflow.keras.models import Sequential,load_model,save_model\n",
        "from tensorflow.keras.layers import TimeDistributed, Conv1D, Activation, Bidirectional, LSTM, MaxPooling1D, Dropout, Flatten, Dense, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.signal import hilbert\n",
        "from scipy.signal import welch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeCNNLSTM:\n",
        "    def __init__(self, input_signal, target_signal):\n",
        "        self.input_signal = input_signal\n",
        "        self.target_signal = target_signal\n",
        "        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "        self.model = Sequential()\n",
        "        self.history = None\n",
        "    def preprocess_signals(self):\n",
        "        self.input_signal = self.scaler.fit_transform(self.input_signal.reshape(-1,1))\n",
        "        self.input_signal = self.input_signal.reshape((-1, 1, 1))\n",
        "        self.target_signal = self.target_signal.reshape((-1, 1, 1))\n",
        "\n",
        "    def build_model(self, initial_learning_rate=0.01):\n",
        "        signal_length = self.input_signal.shape[1]\n",
        "        self.model.add(Conv1D(filters=64, kernel_size=10, input_shape=(signal_length, 1), padding='same'))\n",
        "        self.model.add(Activation('relu'))\n",
        "        pool_size = min(2, signal_length)\n",
        "        self.model.add(MaxPooling1D(pool_size=pool_size))\n",
        "        self.model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "        self.model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "        self.model.add(Dropout(0.2))\n",
        "        self.model.add(TimeDistributed(Dense(units=64, activation='relu')))\n",
        "        self.model.add(LSTM(units=64, return_sequences=True))\n",
        "        self.model.add(Dense(1, activation='softmax', name='output'))\n",
        "        optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "        self.model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
        "        self.model.summary()\n",
        "\n",
        "    def train_model(self, epochs=10, batch_size=64, validation_data=None):\n",
        "        if validation_data:\n",
        "            validation_data = (validation_data[0].reshape((-1, 1, 1)), validation_data[1].reshape((-1, 1, 1)))\n",
        "        def lr_scheduler(epoch, lr):\n",
        "            if epoch % 10 == 0 and epoch != 0:\n",
        "                return lr * 0.9\n",
        "            else:\n",
        "                return lr\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            self.input_signal, self.target_signal,\n",
        "            epochs=epochs, batch_size=batch_size, verbose=1,\n",
        "            validation_data=validation_data,\n",
        "            callbacks=[LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "        )\n",
        "\n",
        "    def plot_loss(self):\n",
        "        if self.history is not None:\n",
        "            plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "            if 'val_loss' in self.history.history:\n",
        "                plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Loss over Epochs')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    def predict_new_signal(self, new_signal):\n",
        "        new_signal = new_signal.reshape((-1, 1, 1))\n",
        "        prediction = self.model.predict(new_signal)\n",
        "        return prediction\n",
        "\n",
        "    def save_model(self, model_path='/content/drive/MyDrive/thesis/delmodel/Segmentations_model_GLQ.h5'):\n",
        "        self.model.save(model_path)\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "\n",
        "    def load_model(self, model_path='/content/drive/MyDrive/thesis/delmodel/Segmentations_model_GLQ.h5'):\n",
        "        self.model = load_model(model_path)\n",
        "        print(f\"Model loaded from {model_path}\")\n"
      ],
      "metadata": {
        "id": "_ekuNpQfkpX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_and_target_from_folder(data_folder_path, target_folder_path, delimiter=';'):\n",
        "    data_files = glob.glob(os.path.join(data_folder_path, '*.CSV'))\n",
        "    data_files.sort()\n",
        "    data_list = []\n",
        "    target_list = []\n",
        "    target2_list = []\n",
        "    for data_file in data_files:\n",
        "        file_name = os.path.splitext(os.path.basename(data_file))[0]\n",
        "        target_file = os.path.join(target_folder_path, f'{file_name}.CSV')\n",
        "        if os.path.exists(target_file):\n",
        "            df_data = pd.read_csv(data_file, delimiter=delimiter)[1000:9000]\n",
        "            df_target = pd.read_csv(target_file, delimiter=delimiter)[1000:9000]\n",
        "            data_list.append(df_data.filter(like='data').values)\n",
        "            target_list.append(df_target.filter(like='pwave').values)\n",
        "            target2_list.append(df_target.filter(like='swave').values)\n",
        "    data_array = np.concatenate(data_list)\n",
        "    target_array = np.concatenate(target_list)\n",
        "    target2_array = np.concatenate(target2_list)\n",
        "    return data_array, target_array, target2_array\n",
        "data_folder_path = '/content/drive/MyDrive/thesis/filtered'\n",
        "target_folder_path = '/content/drive/MyDrive/thesis/label'\n",
        "data_array, target_array, target2_array = load_data_and_target_from_folder(data_folder_path, target_folder_path)"
      ],
      "metadata": {
        "id": "ibsfyaJioIwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "das=data_array[100000:120000]\n",
        "tar=target_array[100000:120000]\n",
        "plt.plot(das*0.1, label='microtremor')\n",
        "plt.plot(tar, label='p-wave')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F6pZs1-9Uy_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussLegendreQuadrature:\n",
        "    def __init__(self, num_points=3):\n",
        "        self.num_points = num_points\n",
        "        self.points, self.weights = np.polynomial.legendre.leggauss(num_points)\n",
        "\n",
        "    def integrate(self, function):\n",
        "        result = np.sum(self.weights * function(self.points))\n",
        "        return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    def function_to_integrate(x):\n",
        "        return x\n",
        "    num_points = 4\n",
        "    gauss_legendre = GaussLegendreQuadrature(num_points)\n",
        "    x_vals = np.array(das)\n",
        "    cumulative_results = []\n",
        "    cumulative_sum = 0\n",
        "    for x in x_vals:\n",
        "        cumulative_sum += gauss_legendre.integrate(lambda x_val: function_to_integrate(x_val + x))\n",
        "        cumulative_results.append(cumulative_sum)\n",
        "\n",
        "    analytic_signal = hilbert(cumulative_results)\n",
        "    hilbert_gauss_legendre = np.abs(analytic_signal)\n",
        "    analytic_s = hilbert(x_vals)\n",
        "    hilbert_envelope = np.abs(analytic_s)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(4, 1, 1)\n",
        "    plt.plot(x_vals, label='Original Sine Wave')\n",
        "    plt.title('Original Sine Wave Function')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.subplot(4, 1, 2)\n",
        "    plt.plot( np.array(cumulative_results), label='Cumulative Integration ', color='orange')\n",
        "    plt.plot(tar)\n",
        "    plt.title('Cumulative Integration of Sine Wave using Gaussian-Legendre Quadrature')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('Cumulative Amplitude')\n",
        "    plt.legend()\n",
        "    plt.subplot(4, 1, 3)\n",
        "    plt.plot( hilbert_gauss_legendre, label='Hilbert_Gauss Transform Analysis', color='green')\n",
        "    plt.title('Hilbert Transform Analysis')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.subplot(4, 1, 4)\n",
        "    plt.plot( hilbert_envelope, label='Hilbert Transform Analysis', color='green')\n",
        "    plt.title('Hilbert Transform Analysis')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Onf9p7P2-vAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1=np.array(cumulative_results)#np.array(hilbert_gauss_legendre)\n",
        "#data_2=target_array[:563301]\n",
        "data_3=np.array(tar)\n",
        "cnn_lstm_auto_filter = DeCNNLSTM(data_1, data_3)\n",
        "cnn_lstm_auto_filter.preprocess_signals()\n",
        "cnn_lstm_auto_filter.build_model()\n"
      ],
      "metadata": {
        "id": "RAaniTHGliQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_1=np.array(cumulative_results)#np.array(hilbert_gauss_legendre)\n",
        "#data_2=target_array[:563301]\n",
        "data_3=np.array(tar)\n",
        "cnn_lstm_auto_filter = DeCNNLSTM(data_1, data_3)\n",
        "cnn_lstm_auto_filter.preprocess_signals()\n",
        "cnn_lstm_auto_filter.build_model()\n"
      ],
      "metadata": {
        "id": "_5HHiRb7clTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_lstm_auto_filter.train_model(epochs=200, batch_size=512)\n",
        "cnn_lstm_auto_filter.plot_loss()\n",
        "cnn_lstm_auto_filter.save_model()"
      ],
      "metadata": {
        "id": "RzTmYaKO93x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_1=np.array(hilbert_gauss_legendre)\n",
        "#data_2=target_array[:563301]\n",
        "data_3=np.array(tar)\n",
        "model1 = DeCNNLSTM(data_1, data_3)\n",
        "model1.load_model('/content/drive/MyDrive/thesis/delmodel/Segmentations_model_hilbert.h5')\n",
        "new_signal = data_1[:20000]\n",
        "predictions1 = model1.predict_new_signal(new_signal)\n",
        "model2 = DeCNNLSTM(data_1, data_3)\n",
        "model2.load_model('/content/drive/MyDrive/thesis/delmodel/Segmentations_model_GLQ.h5')\n",
        "predictions2 = model2.predict_new_signal(new_signal)\n",
        "print(predictions2)\n",
        "print(predictions1)\n",
        "threshold = 3.8940729e-05\n",
        "binary_predictions1 = (predictions1 > threshold).astype(int)\n",
        "binary_predictions2 = (predictions2 > threshold).astype(int)\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(das[:20000]*10, label='Original Signal')\n",
        "plt.plot(predictions1.flatten(), label='s-wave', alpha=0.5)\n",
        "plt.plot(predictions2.flatten(), label='p-wave', alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hYpwKap74sH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ksapAv4kT_2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "filters untuk denoising"
      ],
      "metadata": {
        "id": "F5bkLyILnoP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "perintah untuk menjalankan"
      ],
      "metadata": {
        "id": "a62pgyE8n0KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "untuk training tiap file dalam 1 folder"
      ],
      "metadata": {
        "id": "aAhdULYyoFwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model1 = DeCNNLSTM(data_1, data_3)\n",
        "model1.load_model('/content/drive/MyDrive/thesis/delmodel/Segmentations_model_s-wave.h5')\n",
        "new_signal = data_1[:7000]\n",
        "predictions1 = model1.predict_new_signal(new_signal)\n",
        "\n",
        "model2 = DeCNNLSTM(data_1, data_3)\n",
        "model2.load_model('/content/drive/MyDrive/thesis/delmodel/Segmentations_model.h5')\n",
        "predictions2 = model2.predict_new_signal(new_signal)\n",
        "threshold = 3.8940729e-05\n",
        "binary_predictions1 = (predictions1 > threshold).astype(int)\n",
        "binary_predictions2 = (predictions2 > threshold).astype(int)\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(das[:7000]*50, label='Original Signal')\n",
        "#plt.plot(binary_predictions1.flatten(), label='microtremor', alpha=0.7)\n",
        "plt.plot(binary_predictions2.flatten(), label='p-wave', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "k9OHWwZwRd-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}